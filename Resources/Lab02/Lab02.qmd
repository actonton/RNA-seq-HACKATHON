---
title: 'Lab 2: Week 2'
author: "Hruday"
format: 
  html: 
    #### IMPORTANT ###
    self-contained: true ## Creates a single HTML file as output
    code-fold: show ## Code folding; allows you to show/hide code chunks
    #### USEFUL ###
    code-tools: true ## Includes a menu to show/hide all chunks
    #### OPTIONAL ###
    code-line-numbers: true ## Line numbers in code chunks
    df-print: paged ## Sets how dataframes are automatically printed
    theme: lux ## Controls the font, colours, etc.
    table-of-contents: true ## (Useful) Creates a table of contents!
    number-sections: true ## (Optional) Puts numbers next to heading/subheadings
---

```{r setup, echo=FALSE, message=FALSE}
#Setup
#library(dplyr)
library(tibble)
library(GEOquery) 
library(R.utils)
library(reshape2)
library(ggplot2)
library(limma)
## gse <- getGEO("GSE46474")
##gse <- gse$GSE46474_series_matrix.txt.gz
load("data/GSE46474.RData")
gse
```
# Gene expression data
Kidney transplantation is often the treatment of choice for people with end-stage kidney disease. However, despite advances in the field of transplantation, the proportion of patients who develop graft rejection after a kidney transplant remains high. Understanding the characteristics between stable patients and patients who experience rejection can be one of the ways to improve health outcomes for these people.

In this exercise we will visualise public datasets on kidney transplant patients. These are RNA-sequencing datasets that measure the gene expression of patients’ cells. We will then use the gene expression to predict the patients’ outcome (i.e., stable versus rejection). The public datasets are all taken from the Gene Expression Omnibus database. You can download the dataset by looking up its GSE ID in the database. Information about each dataset, for example, the paper that this data is published in, the experimental protocol, can also be found on the database. We will first focus on how to analyse the data from “GSE46474”. This dataset contains the gene expression profiles of 40 blood samples. Of those, 20 patients developed graft rejection and 20 had stable grafts and will be treated as controls. Note, you can go to the package website for installation details.

## Basic Statistics
The actual expression matrix can be extracted using the exprs function. Data about the phenotype of each patient can be extracted using pData function. Similarly, we can extract the featureData object using the fData function.

```{r, message=FALSE}
#Sample code
gse$Outcome <- ifelse(grepl("AR", gse$title), "Rejection", "Stable") #For every data point in gse$title if the title contains AR then it means kidney is rejected, otherwise it is stable. This is stored in a new column called outcome

#table(gse$Outcome)
#table(gse$source_name_ch1)
head(pData(gse)[,1:5])
head(fData(gse)[,1:5])
head(exprs(gse)[,1:5])
eMat = exprs(gse) #converts the gse to matrix with sampleNames as columns and featureData as row names

#Have to be careful because number of columns is actually the patients/samples and number of rows is actually the genes/features
nrow(gse)
ncol(gse)
```

### (a) What is the number of samples in the Rejection class and the Stable class?
20 rejection, 20 stable.

### (b) What is the size of this data matrix?
54613*40=2184520

### (c) [Exploration - optional] The names of these features aren’t “genes” but the ID generated by the biotechnology company that created this platform. In this case, the name of the manufacture is call Affymetrix and the name of their platform is known as Affymetrix Human Genome U133 Plus 2.0 Array. This platform aims to provide the complete coverage of the Human Genome for analysis. Details of the platform can be found at the company’s website. Here, the information gene symbols are stored in the column Gene Symbol and the description of the genes are stored in the column named Gene Title.
- Its just the title column in gse

## Data transformation
### (a) Log transformation is a standard normalisation technique for RNA-seq data. Has our dataset has been log2 transformed? How can we determine this from looking at the data?

```{r, message=FALSE}
shap.per.feature.raw = rep(0, nrow(eMat))
shap.per.feature.log2 = rep(0, nrow(eMat))

for (i in 1:nrow(eMat)){
  shap.per.feature.raw[i] = shapiro.test(eMat[i, ])$p.value
  shap.per.feature.log2[i] = shapiro.test(log2(eMat[i, ]))$p.value
}
print("Percentage of non normal raw features")
sum(shap.per.feature.raw < 0.05)/nrow(eMat)
print("Percentage of non normal log2 features")
sum(shap.per.feature.log2 < 0.05)/nrow(eMat)
```

- To check normalisation we can look at the features and their values across the multiple patients and  shapiro test for normality.
  - If dataset is log2 then most features should follow normality but not be significantly higher than seeing how many features follow normality after log2 applied.
- Looks like log2 was applied in given data as in raw results 20% of the features didnt follow normal distribution but after log2 was applied this only reduced to 15% not following normal distribution. A reduction is expected because applying log would compress distance between points anyway.

### (b) Save a copy of this processed dataset for future usage using write.csv.
```{r, message=FALSE}
write.csv(exprs(gse), "data/GSE46474_expression_matrix.txt")
```

## It is critical that we examine the dataset. Create a series of boxplots to compare the sample distribution among different people. Are there any samples that appear to be significantly different from the others?  Is it necessary to remove any samples (patients)? [Discussion] Can you think of any other graphical techniques to visualise or analyse the data?

```{r}
for (i in 1:5){
  row = tibble(eMat[i, ])
  print(row)
  row$patient = colnames(eMat)
  colnames(row) = c("values", "sampleid")
  bp = ggplot() + geom_boxplot(data=row, aes(y=values)) + xlab(label=paste("Feature: ", rownames(eMat)[i])) + geom_point(data=row, aes(x = 0, y = values, color=sampleid), inherit.aes = FALSE)
  
  print(bp)
}

```

- Not necessary to remove any patients, none of them follow any patterns which suggest they are frequently outliers for a lot of features.

# Association analysis: Finding signals in the data
## T-test within 1 gene
Let us select row 8251 and have a look. This represents the gene expression from a selected gene. Calculate the two sample t-statistic which could be used to test the null hypothesis, that the mean log(expression) for rejection individuals is the same as that for stable individuals. Assume that this t-statistic does indeed have a t-distribution; obtain the p-value for testing this hypothesis. Check for normality of the data using a qqplot.

```{r}
interest.row = tibble(eMat[8251, ])
interest.row$patient = colnames(eMat)
interest.row$outcome = gse$Outcome
colnames(interest.row) = c("gene.val", "patientid", "outcome")

qqnorm(interest.row$gene.val)
qqline(interest.row$gene.val)

rejects = interest.row[which(interest.row$outcome == "Rejection"),]
stable = interest.row[which(interest.row$outcome == "Stable"),]

t.stat = t.test(rejects$gene.val, stable$gene.val, alternative='two.sided', var.equal = TRUE)$statistic

p.val = 2*pt(abs(t.stat), df=(nrow(rejects)-1)+(nrow(stable)-1), lower.tail=FALSE)
```
- QQplot shows points relatively equally spread and following qqline
- The p.val is `r p.val` which is < 0.05 hence we reject the null hypothesis that mean log(expression) is same for stable and rejection individuals.

## DE genes
What are the highly differentially expressed genes between stable and rejection patients? This is the same question as which genes have the most different expression between stable and rejection patients. We can perform a series of t-tests, and the bioinformatics community has developed a package that speeds up this process. We will examine a package called limma which fits a series of linear models $Y=Xβ$ where X is known as the design matrix. The theory and methods behind all this are covered in STAT3022 - Applied linear models.

```{r, message=FALSE}
design <- model.matrix(~Outcome, data = pData(gse)) #Construct design matrix of predictors
#?lmFit shows us that design matrix is matrix where each row is the sample name and the columns are the coefficients to estimate which what we want to estimate is the outcome whether its rejection or stable.
fit <- limma::lmFit(eMat, design=design) #Pass in design matrix and expression matrix
fit <- eBayes(fit) #eBayes takes in the lmFit and performs t-stats for each row in the expression matrix combined with the design matrix, effectively running t-test for each gene

## Without gene symbols
library(DT)
tT <- topTable(fit, n = Inf) 
DT::datatable(tT[1:100, ]) #display the top 100 genes data

## Adding gene symbols
tT <- topTable(fit, genelist=fData(gse)[,"Gene Symbol"], n = 20)
DT::datatable(tT[1:100, ])
```

## Multiple testing
Explore: Run the following code and explore the impact of multiple testing. Convince yourself via simulation.

```{r, message=FALSE}
num.pats = 80
cl = factor(sample(c("Rejection", "Stable"), num.pats, replace=TRUE))
fakeX = matrix(rnorm(10000*80), nrow=10000) #Randomly assign number between -1 and 1 to 80 patients a value, repeat this 10000 times
design <- model.matrix(~ cl + 0) #Convert cl to design
fakefit <- lmFit(fakeX, design)
cont.matrix <- makeContrasts(clRejection - clStable, levels=design)
fakefit2 <- contrasts.fit(fakefit, cont.matrix)
fakefit2 <- eBayes(fakefit2)
```

## Visualising Results
We will examine our analytical results using two types of scatter plots
### MA-plot: logFC (y-axis) vs AveExpr (x-axis) where M stands for Minus and A stands for Add.
```{r, message=FALSE}
#Sample code
## Code for MA plot
df<- topTable(fit, number=nrow(fit), genelist=rownames(gse))
ggplot(df, aes(x = AveExpr, y = logFC))+
    geom_point(aes(colour=-log10(P.Value)), alpha=1/3, size=1) +
    scale_colour_gradient(low="blue",high="red")+
    ylab("log2 fold change") + xlab("Average expression")
    
## Code to extract negative log p-value
p.value = -log10(df$P.Value)
```
### Volcano plot: -log(P.Value) (y-axis) vs logFC (x-axis).
### [Extension] Consider making these plots interactive so that you can locate or identify the genes of interest.

# Building predictive model
## Exploring with PCA
It would be nice if we can visualise our data with a 2D scatter-plot. However, for each patient, there are more than 10000 variables. How are we going to visualise this information for each patient? Similar to Lab 1, we can use a dimension reduction technique called PCA. PCA helps to reduce or summarise the information of a high dimensional object into a lower dimension. We can then use the first and second dimension, as the summary information of the >10000 genes, to plot a 2D scatter-plot. The PCA plot frequently gives us a good indication of how simple or difficult the relevant prediction task is. Please submit this plot for formative feedback and comment (1 sentence only) on what you learn from looking at this graph. Take care to make sure the graphic is properly labelled.

```{r}
eDF = t(data.frame(eMat))
pca.eDF = t(summary(prcomp(eDF))$x)
bind.pca.edf = rbind(eMat, pca.eDF)

outcomes = t(data.frame(c(gse$Outcome) == "Rejection"))
rownames(outcomes) = c('Outcomes')
colnames(outcomes) = colnames(bind.pca.edf)
bind.pca.edf = rbind(bind.pca.edf, outcomes)
bind.pca.edf = data.frame(t(bind.pca.edf))

bind.pca.edf |> ggplot() + geom_point(aes(x=PC1, y=PC2, color=factor(Outcomes))) + theme_bw() + xlab('Principal Component 1') + ylab('Principal Component 2') + scale_color_manual(name="Outcomes", values=c("red", "blue"), labels=c("Stable", "Rejection")) + geom_hline(yintercept=0) + geom_vline(xintercept=0) + ggtitle("PC1 vs PC2")

```
- First 2 components explain most of the variance in data
- The plot shows us that the points are roughly randomly spaced apart with no clear clusters or patterns. This suggests that the data is normally distributed and training models using randomly sampled data might give high accuracy results.

## Prediction Model
Let’s apply the machine learning models we have learnt in the previous week to predict patient rejection status from gene expression data. One of the key challenges of building a classification model for biomedical data is facing the large p small n challenge. This means the number of variables (p) is often a lot more than the number of samples (n) which creates a lot of challenges, we address this in a ad hoc way by selecting a subset of genes (features). The following code will help you get started.

```{r, message=FALSE}
#Sample code
## quick filter to reduce computational time
largevar = apply(gse, 1, var)
ind = which(largevar > quantile(largevar, 0.95))

X = as.matrix(t(exprs(gse[ind,])))
y = gse$Outcome

cvK = 5  # number of CV folds
cv_50acc5_knn = cv_50acc5_svm = cv_50acc5_rf = c()
cv_acc_knn = cv_acc_svm = cv_acc_rf = c()

n_sim = 25 ## number of repeats

for (i in 1:n_sim) {
  cvSets =  cvTools::cvFolds(nrow(X), cvK)  # permute all the data, into 5 folds
  cv_acc_knn = cv_acc_svm = cv_acc_rf = c()
  
  for (j in 1:cvK) {
    test_id = cvSets$subsets[cvSets$which == j]
    X_test = X[test_id, ]
    X_train = X[-test_id, ]
    y_test = factor(y[test_id])
    y_train = y[-test_id]
    
    ## KNN
    fit5 = class::knn(train = X_train, cl = y_train, test = X_test, k = 5)
    #cv_acc_knn[j] = caret::confusionMatrix(fit5, y_test)$overall[1]
    cv_acc_knn[j] = mean(fit5 == y_test)
    
    ## SVM
    svm_res <- e1071::svm(x = X_train, y = as.factor(y_train))
    fit <- predict(svm_res, X_test)
    cv_acc_svm[j] = caret::confusionMatrix(fit, y_test)$overall[1]

    ## RandomForest
    rf_res <- randomForest::randomForest(x = X_train, y = as.factor(y_train))
    fit <- predict(rf_res, X_test)
    cv_acc_rf[j] = caret::confusionMatrix(fit, y_test)$overall[1]
  }
  cv_50acc5_knn <- append(cv_50acc5_knn, mean(cv_acc_knn))
  cv_50acc5_svm <- append(cv_50acc5_svm, mean(cv_acc_svm))
  cv_50acc5_rf <- append(cv_50acc5_rf, mean(cv_acc_rf))
} ## end for

boxplot(list(SVM = cv_50acc5_svm, KNN = cv_50acc5_knn , RF= cv_50acc5_rf ), ylab="CV Accuracy")
```

## Assignment 1 Question

One of the assignment 1 questions will be based on the following task.
Blood vs Biopsy Biomarker.

We estimated the accuracy for our predictive model in graft rejection from a peripheral blood gene expression dataset. However, rejection is a very active process that occurs in the kidney itself. Here we will look at a similar kidney microarray dataset. But instead of genes being isolated and sequenced from blood, they have been sequenced from a kidney biopsy. How do these differ when compared to the peripheral blood models we generated earlier? Can you make a informative plot?

[Optional] What reasons can you think of that account for this difference in model accuracy?

The code below illustrate how to download another dataset. We will also make GSE138043.RData available.

```{r, message=FALSE}
#Sample code
#Setup
library(tibble)
library(GEOquery) 
library(R.utils)
library(reshape2)
library(ggplot2)
library(limma)
#gse <- getGEO("GSE138043")
#gse <- gse$GSE138043_series_matrix.txt.gz

#This is for gse138043 dataset
load("data/GSE138043.RData")

library(preprocessCore)
exprs(gse) <- normalize.quantiles(exprs(gse)) # Normalise Data

#Find and make outcomes list
gse$Outcome <- ifelse(grepl("non-AR", gse$characteristics_ch1), "Stable", "Rejection") #For every data point in gse$characteristics_ch1 if the characcteristic is non-AR then it means kidney is stable, otherwise it is rejected This is stored in a new column called outcome

#Now we have the outcomes, we can perform train some models using this and compare the box plots with the one of blood, it will also have a quick filter

## quick filter to reduce computational time
largevar = apply(gse, 1, var)
ind = which(largevar > quantile(largevar, 0.95))

X = as.matrix(t(exprs(gse[ind,])))
y = gse$Outcome

cvK = 5  # number of CV folds
cv_50acc5_knn = cv_50acc5_svm = cv_50acc5_rf = c()
cv_acc_knn = cv_acc_svm = cv_acc_rf = c()

n_sim = 25 ## number of repeats

for (i in 1:n_sim) {
  cvSets =  cvTools::cvFolds(nrow(X), cvK)  # permute all the data, into 5 folds
  cv_acc_knn = cv_acc_svm = cv_acc_rf = c()
  
  for (j in 1:cvK) {
    test_id = cvSets$subsets[cvSets$which == j]
    X_test = X[test_id, ]
    X_train = X[-test_id, ]
    y_test = factor(y[test_id])
    y_train = y[-test_id]
    
    ## KNN
    fit5 = class::knn(train = X_train, cl = y_train, test = X_test, k = 5)
    #cv_acc_knn[j] = caret::confusionMatrix(fit5, y_test)$overall[1]
    cv_acc_knn[j] = mean(as.character(fit5) == as.character(y_test))
    
    ## SVM
    svm_res <- e1071::svm(x = X_train, y = as.factor(y_train))
    fit <- predict(svm_res, X_test)
    cv_acc_svm[j] = mean(as.character(fit) == as.character(y_test))

    ## RandomForest
    rf_res <- randomForest::randomForest(x = X_train, y = as.factor(y_train))
    fit <- predict(rf_res, X_test)
    cv_acc_rf[j] = mean(as.character(fit) == as.character(y_test))
  }
  cv_50acc5_knn <- append(cv_50acc5_knn, mean(cv_acc_knn))
  cv_50acc5_svm <- append(cv_50acc5_svm, mean(cv_acc_svm))
  cv_50acc5_rf <- append(cv_50acc5_rf, mean(cv_acc_rf))
} ## end for

boxplot(list(SVM = cv_50acc5_svm, KNN = cv_50acc5_knn , RF= cv_50acc5_rf ), ylab="CV Accuracy")
```

- We notice that the trend is completely opposite using biopsy data. For example in blood sample models the random forest model had a very large range and interquartile range while using biopsy data the random forest model has a smaller interquartile range and range with a higher mean accuracy also.